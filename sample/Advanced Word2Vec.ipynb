{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Advanced Word2Vec.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMttj9yJqRpJT600Yns9fzr"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"JZpv5Rm240MS"},"source":["checkpoint = tf.compat.v1.train.Saver([tensor_embeddings])\n","checkpoint_path = checkpoint.save(sess=None, global_step=None, save_path=os.path.join('/content/drive/', 'model.ckpt'))\n","\n","projector.visualize_embeddings('/content/drive/', config)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6zIUKBrDMtf3"},"source":["import glo\n","import os\n","import re\n","import platform, sys, smart_open\n","from gensim.test.utils import common_texts, get_tmpfile\n","\n","from gensim.corpora.textcorpus import TextCorpus\n","from gensim.test.utils import datapath\n","from gensim import utils\n","nltk.download('punkt')\n","\n","import sklearn\n","from sklearn.manifold import TSNE\n","import matplotlib\n","import matplotlib.pyplot as plt\n","pd.options.display.max_colwidth = 200\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5Px3-BEn5ZW4"},"source":["# in command prompt: tensorboard --logdir=\"C:\\\\Users\\\\alwer\\Desktop\\\\Coding Working Group\\\\\" --host localhost --port 8088\n","file_writer = tf.summary.FileWriter('/path/to/logs', sess.graph)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KAtJJNCOAA1G"},"source":["# See more advance tuto on Saver object\n","tf.global_variables_initializer().run()\n","saver = tf.train.Saver()\n","saver.save(sess, save_path=os.path.join(log_dir, 'model.ckpt'), global_step=None)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nKwkF91bAI6-"},"source":["tensorboard --logdir=\"\".\n","\n","sess = tf.Session()\n","summary_writer = tf.summary.FileWriter('logs', graph=sess.graph)\n","\n","embedding = tf.Variable(model.wv.vectors, name='embeddings')\n","\n","LOG_DIR = 'log/'\n","metadata = os.path.join(LOG_DIR, 'metadata.tsv')\n","# Mention label name\n","metadata_file.write(\"Label1\\tLabel2\\n\")\n","with open(metadata, 'w') as metadata_file:\n","    for data in whatever_object:\n","        metadata_file.write('%s\\t%s\\n' % data.label1, data.label2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"diYf14lCAVWI"},"source":["\n","# Save the weights we want to analyse as a variable. Note that the first\n","# value represents any unknown word, which is not in the metadata, so\n","# we will remove that value.\n","weights = tf.Variable(model.layers[0].get_weights()[0][1:])\n","# Create a checkpoint from embedding, the filename and key are\n","# name of the tensor.\n","checkpoint = tf.train.Checkpoint(embedding=weights)\n","checkpoint.save(os.path.join(log_dir, \"embedding.ckpt\"))\n","\n","# Set up config\n","config = projector.ProjectorConfig()\n","embedding = config.embeddings.add()\n","# The name of the tensor will be suffixed by `/.ATTRIBUTES/VARIABLE_VALUE`\n","embedding.tensor_name = \"embedding/.ATTRIBUTES/VARIABLE_VALUE\"\n","embedding.metadata_path = 'metadata.tsv'\n","projector.visualize_embeddings(log_dir, config)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AcFmjrzSAYs6"},"source":["file_writer = tf.summary.FileWriter('/path/to/logs', sess.graph)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sxgsYAg1AbN8"},"source":["tensorboard --logdir path/to/logs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2J-klxRhAN96"},"source":["import tensorflow.compat.v1 as tf\n","tf.disable_v2_behavior()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5X6GcBNrrnnG"},"source":["# Text Cleaning\n","\n"]},{"cell_type":"code","metadata":{"id":"q4yWe4enPASl"},"source":["### import NLTK corpus as corpus2\n","nltk.download('brown')\n","from nltk.corpus import brown\n","text1 = brown.sents()\n","corpus2 = nltk.word_tokenize(text1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OC3TPF-D3DaU"},"source":["###### Further Cleaning (Alternate)\n","import numpy as np\n","\n","wpt = nltk.WordPunctTokenizer()\n","stop_words = nltk.corpus.stopwords.words('english')\n","\n","def normalize_document(corpus2):\n","    # lower case and remove special characters\\whitespaces\n","    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n","    doc = doc.lower()\n","    doc = doc.strip()\n","    # tokenize document\n","    tokens = wpt.tokenize(doc)\n","    # filter stopwords out of document\n","    filtered_tokens = [token for token in tokens if token not in stop_words]\n","    # re-create document from filtered tokens\n","    doc = ' '.join(filtered_tokens)\n","    return doc\n","\n","sents_clean = np.vectorize(normalize_document)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6KRar7sp2Vvs"},"source":["# Tensorboard Extra"]},{"cell_type":"code","metadata":{"id":"XsHpDpmg2VQY"},"source":["config = projector.ProjectorConfig()\n","embedding = config.embeddings.add()\n","embedding.tensor_name = 'embeddings'\n","embedding.metadata_path = '/content/gdrive/My Drive/metadata.tsv'\n","\n","tensor_embeddings = tf.Variable(model.wv.vectors, name='embeddings')\n","\n","checkpoint = tf.compat.v1.train.Saver([tensor_embeddings])\n","checkpoint_path = checkpoint.save(sess=None, global_step=None, save_path=os.path.join('/content/gdrive/My Drive/', \"model.ckpt\"))\n","\n","projector.visualize_embeddings('/content/gdrive/My Drive/', config)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iomdX3N02eO5"},"source":["# Multiple Documents"]},{"cell_type":"code","metadata":{"id":"AT0jbcYU2fSG"},"source":["### Import Documents into Corpus\n","import glob\n","import os\n","file_list = glob.glob(os.path.join(os.getcwd(),\"C:\\\\Users\\\\alwer\\\\Desktop\\\\Coding Working Group\\\\test\", \"*.txt\"))\n","\n","corpus = []\n","\n","for file_path in file_list:\n","    with open(file_path, encoding = \"utf8\", errors='ignore') as f_input:\n","        corpus.append(f_input.read())\n","        \n","print(corpus[:2])  \n","\n","# Change File List to Title - to work on\n","my_dir = \"C:\\\\Users\\\\alwer\\\\Desktop\\\\Coding Working Group\\\\test\"\n","filelist = []\n","filesList = []\n","os.chdir( my_dir )\n","\n","# Step 2: Build up list of files:\n","for files in glob.glob(\"*.txt\"):\n","    fileName, fileExtension = os.path.splitext(files)\n","    filelist.append(fileName) #filename without extension\n","    \n","# Import Documents into Dataframe\n","corpus = np.array(corpus)\n","corpus_df = pd.DataFrame({'Title': filelist, 'Text': corpus})\n","corpus_df = corpus_df[['Title', 'Text']]\n","corpus_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ha5Ar6xh2raQ"},"source":["### Visualize Document Corpus Embeddings\n","from sklearn.manifold import TSNE\n","\n","words = w2v_model.wv.index2word\n","wvs = w2v_model.wv[words]\n","\n","tsne = TSNE(n_components=2, random_state=0, n_iter=250, perplexity=2)\n","np.set_printoptions(suppress=True)\n","T = tsne.fit_transform(wvs)\n","labels = words\n","\n","plt.figure(figsize=(12, 6))\n","plt.scatter(T[:, 0], T[:, 1], c='orange', edgecolors='r')\n","for label, x, y in zip(labels, T[:, 0], T[:, 1]):\n","    plt.annotate(label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BKfov1zz2t4J"},"source":["def average_word_vectors(words, model, vocabulary, num_features):\n","    \n","    feature_vector = np.zeros((num_features,),dtype=\"float64\")\n","    nwords = 0.\n","    \n","    for word in words:\n","        if word in vocabulary: \n","            nwords = nwords + 1.\n","            feature_vector = np.add(feature_vector, model[word])\n","    \n","    if nwords:\n","        feature_vector = np.divide(feature_vector, nwords)\n","        \n","    return feature_vector\n","    \n","def averaged_word_vectorizer(corpus, model, num_features):\n","    vocabulary = set(model.wv.index2word)\n","    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n","                    for tokenized_sentence in corpus]\n","    return np.array(features)\n","\n","\n","# get document level embeddings\n","w2v_feature_array = averaged_word_vectorizer(corpus=corpus, model=w2v_model,\n","                                             num_features=feature_size)\n","pd.DataFrame(w2v_feature_array)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ce68Jle-2v4m"},"source":["from sklearn.cluster import AffinityPropagation\n","\n","ap = AffinityPropagation()\n","ap.fit(w2v_feature_array)\n","cluster_labels = ap.labels_\n","cluster_labels = pd.DataFrame(cluster_labels, columns=['ClusterLabel'])\n","pd.concat([corpus_df, cluster_labels], axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fYRMKnwI2xwL"},"source":["# GLOVE"]},{"cell_type":"code","metadata":{"id":"MTc_sI_G2y2j"},"source":["##### Import GLoVe (Untested)\n","\n","from gensim.test.utils import datapath, get_tmpfile\n","from gensim.models import KeyedVectors\n","from gensim.scripts.glove2word2vec import glove2word2vec\n","\n","glove_file = datapath('test_glove.txt')\n","tmp_file = get_tmpfile(\"test_word2vec.txt\")\n","\n","_ = glove2word2vec(glove_file, tmp_file)\n","\n","model = KeyedVectors.load_word2vec_format(tmp_file"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rvK6OyIz24A3"},"source":["# KeyedVectors"]},{"cell_type":"code","metadata":{"id":"6AiY6A_g20w7"},"source":["\n","\n","from gensim.models import KeyedVectors\n","path = get_tmpfile(\"wordvectors.kv\")\n","model.wv.save(path)\n","model.wv.save(\"model.wv\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RQu7pwm9223Z"},"source":["filename = \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MF2houHq25O6"},"source":["wmodel = KeyedVectors.load_word2vec_format(filename, binary=True)\n","v = KeyedVectors.load(\"model.wv\", mmap='r')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fsK6XZRs27vt"},"source":["from gensim.scripts.glove2word2vec import glove2word2vec\n","glove_input_file = \"glove.txt\"\n","word2vec_output_file = \"word2vec.txt\"\n","glove2word2vec(glove_input_file, word2vec_output_file)\n","m = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tKKLpDbrrqqb"},"source":["# Comparing Corpora"]},{"cell_type":"code","metadata":{"id":"KtRgqPAbrqKS"},"source":["vector2 = model_cbow.wv['web']\n","print(vector2)\n","vector1 = model_skip.wv['web']\n","print(vector1)\n","vector = model_skip_train.wv['web']\n","print(vector)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rWXVjxur3GZa"},"source":["# Doc2Vec (Test)"]},{"cell_type":"code","metadata":{"id":"tXjExlGW3ILE"},"source":["from gensim.test.utils import common_texts\n","from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n","\n","documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(corpus_df)]\n","model = Doc2Vec(documents, vector_size=5, window=2, min_count=1, workers=4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"80AFnYQg3JYx"},"source":["from gensim.test.utils import get_tmpfile\n","\n","fname = get_tmpfile(\"my_doc2vec_model\")\n","\n","model.save(fname)\n","model = Doc2Vec.load(fname)  # you can continue training with the loaded model!\n","\n","model.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HRLx2cy63KhT"},"source":["vector = model.infer_vector([\"system\", \"response\"])\n","print(vector)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6jWw-wUn3MlJ"},"source":["##https://praveenbezawada.com/2018/01/25/document-similarity-using-gensim-dec2vec/\n","\n","class TaggedDocumentIterator(object):\n","    def __init__(self, doc_list, labels_list):\n","        self.labels_list = labels_list\n","        self.doc_list = doc_list\n","    def __iter__(self):\n","        for idx, doc in enumerate(self.doc_list):\n","            yield TaggedDocument(words=doc.split(), tags=[self.labels_list[idx]])\n"," \n","docLabels = list(corpus_df['Title'])\n","data = list(corpus_df['Text'])\n","sentences = TaggedDocumentIterator(data, docLabels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DBmjtlKD3ONo"},"source":["model = Doc2Vec(size=100, window=10, min_count=5, workers=11,alpha=0.025, iter=20)\n","model.build_vocab(sentences)\n","model.train(sentences,total_examples=model.corpus_count, epochs=model.iter)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B6e_nxqp3PpG"},"source":["# Store the model to mmap-able files\n","model.save('/tmp/model_docsimilarity.doc2vec')\n","# Load the model\n","model = Doc2Vec.load('/tmp/model_docsimilarity.doc2vec')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3mpsKf4-3TW9"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1YI5fsrV3TZT"},"source":["def test_predict():\n","    #Select a random document for the document dataset\n","    rand_int = np.random.randint(0, corpus_df.shape[0])\n","    print ('Random int {}'.format(rand_int))\n","    test_corpus_df = corpus_df.iloc[rand_int]['info']\n","    label = corpus_df.iloc[rand_int, corpus_df.columns.get_loc('problemReportId')]\n"," \n","    #Clean the document using the utility functions used in train phase\n","    test_corpus_df = default_clean(test_corpus_df)\n","    test_corpus_df = stop_and_stem(test_corpus_df, stem=False)\n"," \n","    #Convert the corpus_df document into a list and use the infer_vector method to get a vector representation for it\n","    new_doc_words = test_corpus_df.split()\n","    new_doc_vec = model.infer_vector(new_doc_words, steps=50, alpha=0.25)\n"," \n","    #use the most_similar utility to find the most similar documents.\n","    similars = model.docvecs.most_similar(positive=[new_doc_vec])\n","test_predict()"],"execution_count":null,"outputs":[]}]}