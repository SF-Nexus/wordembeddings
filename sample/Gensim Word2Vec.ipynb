{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"},"colab":{"name":"Gensim Word2Vec.ipynb","provenance":[{"file_id":"https://github.com/hawc2/Vector-Space-Modeling/blob/master/Gensim%20Word2Vec.ipynb","timestamp":1610131637512}],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"S9zDiMgZUuZW"},"source":["#EDITS - change model_skip to skip_model - find replace\n","#create embezzing size variables for each corpus"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OBnhx9Y7UuZc"},"source":["##### Importing Packages\n","import gensim\n","from gensim.test.utils import common_texts, get_tmpfile\n","from gensim.models import Word2Vec\n","\n","from gensim.corpora.textcorpus import TextCorpus\n","from gensim.test.utils import datapath\n","from gensim import utils\n","\n","import numpy as np\n","import pandas as pd\n","import nltk\n","import glob\n","import os\n","import re\n","import platform, sys, smart_open\n","\n","import sklearn\n","from sklearn.manifold import TSNE\n","import matplotlib\n","import matplotlib.pyplot as plt\n","pd.options.display.max_colwidth = 200\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zEG1tGsqUuZd"},"source":["### import NLTK corpus\n","#nltk.download('brown')\n","from nltk.corpus import brown\n","corpus1 = brown.sents()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-kalWqAFUuZe"},"source":["### open text file\n","path = \"C:\\\\Users\\\\alwer\\\\Desktop\\\\Coding Working Group\\\\corpus.txt\"\n","file = open(path, encoding = \"utf8\", errors='ignore')\n","text = file.read()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jJWw6GXKUuZf"},"source":["### Tokenizing Corpus\n","corpus2 = nltk.sent_tokenize(text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AUr8vd8NUuZf"},"source":["##### Building Your Model - CBOW\n","model_cbow = Word2Vec(corpus1, size=100, window=5, min_count=1, workers=12, iter=2)\n","model_cbow.save(\"word2vec_cbow.model\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wBNfiC1GUuZg"},"source":["##### Building Your Model - Skip-gram\n","model_skip = Word2Vec(corpus1, size=100, window=10, min_count=1, workers=12, iter=100,sg=1)\n","model_skip.save(\"word2vec_skip.model\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SbJXJn2GUuZg"},"source":["##### Training Pre-Loaded Model Further\n","model_skip_train = Word2Vec.load(\"word2vec_skip.model\")\n","model_skip_train.train(corpus2, total_examples=1, epochs=2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B1ZKTkORUuZh"},"source":["###### Exploring Your Model\n","vector2 = model_cbow.wv['web']\n","print(vector2)\n","vector1 = model_skip.wv['web']\n","print(vector1)\n","vector = model_skip_train.wv['web']\n","print(vector)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G3bcycDXUuZh"},"source":["#### Similar Words\n","print(model_skip.similarity('computer', 'human'))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B79qJpIhUuZi"},"source":["print(model_skip.similarity('dog', 'human'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"33mPJuZVUuZk"},"source":["### Similar words\n","similar_words = {search_term: [item[0] for item in model_skip.wv.most_similar([search_term], topn=10)]\n","                  for search_term in ['god', 'table', 'computer', 'human', 'dog', 'plant', 'love','hate']}\n","similar_words"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tHAeM4WWUuZl"},"source":["#### similarity b/t two words\n","# https://github.com/laurenfklein/emory-qtm340/blob/master/notebooks/class12-word-vectors-complete.ipynb\n","print(model_skip.wv.similarity(w1=\"freedom\",w2=\"justice\"))\n","print(model_skip.wv.similarity(w1=\"freedom\",w2=\"slavery\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XvNcyvU5UuZl"},"source":["### Doesn't Match\n","model_cbow.doesnt_match(\"woman ovarian brain\".split())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"00vbpHDEUuZl"},"source":["### Most Similar\n","model_cbow.most_similar(positive=[\"model\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2rnEb_qkUuZm"},"source":["###### analogies\n","# format is: \"man is to king as woman is to ???\"\n","model_skip.wv.most_similar(positive=['woman', 'king'], negative=['man'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wZDgssEUUuZm"},"source":["###### Visualize Similar Words\n","words = sum([[k] + v for k, v in similar_words.items()], [])\n","wvs = model_skip.wv[words]\n","\n","tsne = TSNE(n_components=2, random_state=0, n_iter=10000, perplexity=2)\n","np.set_printoptions(suppress=True)\n","T = tsne.fit_transform(wvs)\n","\n","labels = words\n","\n","plt.figure(figsize=(14, 8))\n","plt.scatter(T[:, 0], T[:, 1], c='orange', edgecolors='r')\n","for label, x, y in zip(labels, T[:, 0], T[:, 1]):\n","    plt.annotate(label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zqytSiLxUuZm"},"source":["# Tensorboard Projections\n","from tensorboard.plugins import projector\n","import tensorflow as tf\n","\n","import logging\n","\"C:\\\\Users\\\\alwer\\Desktop\\\\Coding Working Group\\\\\"\n","# For displaying gensim logs\n","logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)\n","\n","weights     = model_skip.wv.vectors\n","index_words = model_skip.wv.index2word\n","\n","vocab_size    = weights.shape[0]\n","embedding_dim = weights.shape[1]\n","\n","print('Shape of weights:', weights.shape)\n","print('Vocabulary size: %i' % vocab_size)\n","print('Embedding size: %i'  % embedding_dim)\n","\n","with open(os.path.join(\"C:\\\\Users\\\\alwer\\Desktop\\\\Coding Working Group\\\\\",'metadata.tsv'), 'w') as f:\n","    f.writelines(\"\\n\".join(index_words))  \n","\n","config = projector.ProjectorConfig()\n","embedding = config.embeddings.add()\n","embedding.tensor_name = 'embeddings'\n","embedding.metadata_path = './metadata.tsv'\n","projector.visualize_embeddings(\"C:\\\\Users\\\\alwer\\Desktop\\\\Coding Working Group\\\\\", config)\n","\n","tensor_embeddings = tf.Variable(model_skip.wv.vectors, name='embeddings')\n","\n","checkpoint = tf.compat.v1.train.Saver([tensor_embeddings])\n","checkpoint_path = checkpoint.save(sess=None, global_step=None, save_path=os.path.join(\"C:\\\\Users\\\\alwer\\Desktop\\\\Coding Working Group\\\\\", \"model.ckpt\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gQbAfiOFUuZm"},"source":["%reload_ext tensorboard\n","%tensorboard --logdir=logs\n","#default: http://localhost:6006/\n","# in command prompt: tensorboard --logdir=\"C:\\\\Users\\\\alwer\\Desktop\\\\Coding Working Group\\\\\" --host localhost --port 8088\n","# go to http://localhost:8088 "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"apiPrDwzUuZn"},"source":["# Tensorboard Projections 2\n","from tensorboard.plugins import projector\n","import tensorflow as tf\n","\n","weights     = model_cbow.wv.vectors\n","index_words = model_cbow.wv.index2word\n","\n","vocab_size    = weights.shape[0]\n","embedding_dim = weights.shape[1]\n","\n","print('Shape of weights:', weights.shape)\n","print('Vocabulary size: %i' % vocab_size)\n","print('Embedding size: %i'  % embedding_dim)\n","\n","with open(os.path.join(\"C:\\\\Users\\\\alwer\\Desktop\\\\Coding Working Group\\\\\",'metadata.tsv'), 'w') as f:\n","    f.writelines(\"\\n\".join(index_words))  \n","\n","config = projector.ProjectorConfig()\n","embedding = config.embeddings.add()\n","embedding.tensor_name = 'embeddings'\n","embedding.metadata_path = './metadata.tsv'\n","projector.visualize_embeddings(\"C:\\\\Users\\\\alwer\\Desktop\\\\Coding Working Group\\\\\", config)\n","\n","tensor_embeddings = tf.Variable(model_cbow.wv.vectors, name='embeddings')\n","\n","checkpoint = tf.compat.v1.train.Saver([tensor_embeddings])\n","checkpoint_path = checkpoint.save(sess=None, global_step=None, save_path=os.path.join(\"C:\\\\Users\\\\alwer\\Desktop\\\\Coding Working Group\\\\\", \"model1.ckpt\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PDR9zyB6UuZn"},"source":["#%load_ext tensorboard\n","%reload_ext tensorboard\n","%tensorboard --logdir=logs\n","#default: http://localhost:6006/\n","# in command prompt: tensorboard --logdir=\"C:\\\\Users\\\\alwer\\Desktop\\\\Coding Working Group\\\\\" --host localhost --port 6006\n","# go to http://localhost:6006 "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GAIoL67BUuZn"},"source":["###### Word Embeddings of Multiple Documents"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wBm9DglGUuZo"},"source":["### Import Documents into Corpus\n","import glob\n","import os\n","file_list = glob.glob(os.path.join(os.getcwd(),\"C:\\\\Users\\\\alwer\\\\Desktop\\\\Coding Working Group\\\\test\", \"*.txt\"))\n","\n","corpus = []\n","\n","for file_path in file_list:\n","    with open(file_path, encoding = \"utf8\", errors='ignore') as f_input:\n","        corpus.append(f_input.read())\n","        \n","print(corpus[:2])  \n","\n","# Change File List to Title - to work on\n","my_dir = \"C:\\\\Users\\\\alwer\\\\Desktop\\\\Coding Working Group\\\\test\"\n","filelist = []\n","filesList = []\n","os.chdir( my_dir )\n","\n","# Step 2: Build up list of files:\n","for files in glob.glob(\"*.txt\"):\n","    fileName, fileExtension = os.path.splitext(files)\n","    filelist.append(fileName) #filename without extension\n","    \n","# Import Documents into Dataframe\n","corpus = np.array(corpus)\n","corpus_df = pd.DataFrame({'Title': filelist, 'Text': corpus})\n","corpus_df = corpus_df[['Title', 'Text']]\n","corpus_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QE8F-YVcUuZo"},"source":["### Set values for various parameters\n","feature_size = 100    # Word vector dimensionality  \n","window_context = 10          # Context window size                                                                                    \n","min_word_count = 1   # Minimum word count                        \n","sample = 1e-3   # Downsample setting for frequent words\n","\n","w2v_model = Word2Vec(corpus, size=feature_size, \n","                          window=window_context, min_count=min_word_count,\n","                          sample=sample, iter=50, sg=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JiKuuEUFUuZo"},"source":["### Visualize Document Corpus Embeddings\n","from sklearn.manifold import TSNE\n","\n","words = w2v_model.wv.index2word\n","wvs = w2v_model.wv[words]\n","\n","tsne = TSNE(n_components=2, random_state=0, n_iter=250, perplexity=2)\n","np.set_printoptions(suppress=True)\n","T = tsne.fit_transform(wvs)\n","labels = words\n","\n","plt.figure(figsize=(12, 6))\n","plt.scatter(T[:, 0], T[:, 1], c='orange', edgecolors='r')\n","for label, x, y in zip(labels, T[:, 0], T[:, 1]):\n","    plt.annotate(label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MgxK46McUuZo"},"source":["def average_word_vectors(words, model, vocabulary, num_features):\n","    \n","    feature_vector = np.zeros((num_features,),dtype=\"float64\")\n","    nwords = 0.\n","    \n","    for word in words:\n","        if word in vocabulary: \n","            nwords = nwords + 1.\n","            feature_vector = np.add(feature_vector, model[word])\n","    \n","    if nwords:\n","        feature_vector = np.divide(feature_vector, nwords)\n","        \n","    return feature_vector\n","    \n","def averaged_word_vectorizer(corpus, model, num_features):\n","    vocabulary = set(model.wv.index2word)\n","    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n","                    for tokenized_sentence in corpus]\n","    return np.array(features)\n","\n","\n","# get document level embeddings\n","w2v_feature_array = averaged_word_vectorizer(corpus=corpus, model=w2v_model,\n","                                             num_features=feature_size)\n","pd.DataFrame(w2v_feature_array)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y60K26HmUuZp"},"source":["from sklearn.cluster import AffinityPropagation\n","\n","ap = AffinityPropagation()\n","ap.fit(w2v_feature_array)\n","cluster_labels = ap.labels_\n","cluster_labels = pd.DataFrame(cluster_labels, columns=['ClusterLabel'])\n","pd.concat([corpus_df, cluster_labels], axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6eYIOu_ZUuZp"},"source":["##### Import GLoVe (Untested)\n","\n","from gensim.test.utils import datapath, get_tmpfile\n","from gensim.models import KeyedVectors\n","from gensim.scripts.glove2word2vec import glove2word2vec\n","\n","glove_file = datapath('test_glove.txt')\n","tmp_file = get_tmpfile(\"test_word2vec.txt\")\n","\n","_ = glove2word2vec(glove_file, tmp_file)\n","\n","model = KeyedVectors.load_word2vec_format(tmp_file"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZLxvBjw8UuZp"},"source":["#### KeyedVectors\n","\n","from gensim.models import KeyedVectors\n","path = get_tmpfile(\"wordvectors.kv\")\n","model.wv.save(path)\n","model.wv.save(\"model.wv\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Dx36FyivUuZp"},"source":["filename = \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h8fvaKDFUuZp"},"source":["model = KeyedVectors.load_word2vec_format(filename, binary=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P4y79PJSUuZp"},"source":["wv = KeyedVectors.load(\"model.wv\", mmap='r')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xYjFAQRGUuZq"},"source":["vector = wv['computer']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1aGjhGS9UuZq"},"source":["from gensim.scripts.glove2word2vec import glove2word2vec\n","glove_input_file = \"glove.txt\"\n","word2vec_output_file = \"word2vec.txt\"\n","glove2word2vec(glove_input_file, word2vec_output_file)\n","m = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2SfbCob0UuZq"},"source":["###### Further Cleaning (Alternate)\n","import numpy as np\n","\n","wpt = nltk.WordPunctTokenizer()\n","stop_words = nltk.corpus.stopwords.words('english')\n","\n","def normalize_document(corpus2):\n","    # lower case and remove special characters\\whitespaces\n","    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n","    doc = doc.lower()\n","    doc = doc.strip()\n","    # tokenize document\n","    tokens = wpt.tokenize(doc)\n","    # filter stopwords out of document\n","    filtered_tokens = [token for token in tokens if token not in stop_words]\n","    # re-create document from filtered tokens\n","    doc = ' '.join(filtered_tokens)\n","    return doc\n","\n","sents_clean = np.vectorize(normalize_document)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J35Va2o4UuZq"},"source":["##### Doc2Vec (Test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L8a69EuTUuZq"},"source":["from gensim.test.utils import common_texts\n","from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n","\n","documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(corpus_df)]\n","model = Doc2Vec(documents, vector_size=5, window=2, min_count=1, workers=4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"msPex_OJUuZr"},"source":["from gensim.test.utils import get_tmpfile\n","\n","fname = get_tmpfile(\"my_doc2vec_model\")\n","\n","model.save(fname)\n","model = Doc2Vec.load(fname)  # you can continue training with the loaded model!\n","\n","model.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E8ztSapfUuZr"},"source":["vector = model.infer_vector([\"system\", \"response\"])\n","print(vector)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"90R0SN_nUuZr"},"source":["##https://praveenbezawada.com/2018/01/25/document-similarity-using-gensim-dec2vec/\n","\n","class TaggedDocumentIterator(object):\n","    def __init__(self, doc_list, labels_list):\n","        self.labels_list = labels_list\n","        self.doc_list = doc_list\n","    def __iter__(self):\n","        for idx, doc in enumerate(self.doc_list):\n","            yield TaggedDocument(words=doc.split(), tags=[self.labels_list[idx]])\n"," \n","docLabels = list(corpus_df['Title'])\n","data = list(corpus_df['Text'])\n","sentences = TaggedDocumentIterator(data, docLabels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jSeSWZQCUuZs"},"source":["model = Doc2Vec(size=100, window=10, min_count=5, workers=11,alpha=0.025, iter=20)\n","model.build_vocab(sentences)\n","model.train(sentences,total_examples=model.corpus_count, epochs=model.iter)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tmuot5KyUuZs"},"source":["# Store the model to mmap-able files\n","model.save('/tmp/model_docsimilarity.doc2vec')\n","# Load the model\n","model = Doc2Vec.load('/tmp/model_docsimilarity.doc2vec')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6d-43ItEUuZs"},"source":["# Open Text File\n","import io\n","with io.open(\"corpus.txt\",'r',encoding='utf8',errors='ignore') as f:\n","   text = f.read()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P109DlcvUuZs"},"source":["def test_predict():\n","    #Select a random document for the document dataset\n","    rand_int = np.random.randint(0, corpus_df.shape[0])\n","    print ('Random int {}'.format(rand_int))\n","    test_corpus_df = corpus_df.iloc[rand_int]['info']\n","    label = corpus_df.iloc[rand_int, corpus_df.columns.get_loc('problemReportId')]\n"," \n","    #Clean the document using the utility functions used in train phase\n","    test_corpus_df = default_clean(test_corpus_df)\n","    test_corpus_df = stop_and_stem(test_corpus_df, stem=False)\n"," \n","    #Convert the corpus_df document into a list and use the infer_vector method to get a vector representation for it\n","    new_doc_words = test_corpus_df.split()\n","    new_doc_vec = model.infer_vector(new_doc_words, steps=50, alpha=0.25)\n"," \n","    #use the most_similar utility to find the most similar documents.\n","    similars = model.docvecs.most_similar(positive=[new_doc_vec])\n","test_predict()"],"execution_count":null,"outputs":[]}]}